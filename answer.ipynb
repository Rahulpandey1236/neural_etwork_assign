{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.What is deep learning, and how is it connected to artificial intelligence\"\n",
    "Deep learning is a subset of machine learning, which itself is a branch of artificial intelligence (AI). It involves training algorithms, particularly neural networks, to recognize patterns and make decisions by processing large amounts of data. In deep learning, the model consists of multiple layers of interconnected nodes, enabling it to learn complex representations of data. It's used for tasks like image and speech recognition, language processing, and autonomous driving. Deep learning is a powerful tool within AI, allowing machines to perform tasks that traditionally required human intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.What is a neural network, and what are the different types of neural networks!\n",
    "A **neural network** is a computational model inspired by the human brain, designed to recognize patterns and make decisions based on data. It consists of layers of interconnected nodes (or \"neurons\"), where each node processes input data and passes the output to the next layer. Neural networks are used for tasks like classification, regression, and pattern recognition.\n",
    "\n",
    "### Types of Neural Networks:\n",
    "1. **Feedforward Neural Networks (FNN):** The simplest type, where data flows in one direction from input to output without loops.\n",
    "2. **Convolutional Neural Networks (CNN):** Primarily used for image and video recognition, CNNs apply convolutional layers to detect spatial hierarchies.\n",
    "3. **Recurrent Neural Networks (RNN):** Designed for sequential data, RNNs have loops that allow information to persist, making them ideal for tasks like language modeling or time-series analysis.\n",
    "4. **Generative Adversarial Networks (GAN):** Consist of two networks (generator and discriminator) that compete with each other, often used in image generation and data augmentation.\n",
    "5. **Radial Basis Function Networks (RBFN):** Uses radial basis functions as activation functions, commonly applied in classification and function approximation tasks.\n",
    "\n",
    "These different types are designed to address specific tasks and improve the learning process in AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 What is the mathematical structure of a neural network!\n",
    "Input Layer: Receives raw data (e.g., features of an image). Each input corresponds to a feature in the data.\n",
    "\n",
    "Weights (W): Each connection between neurons has an associated weight that determines the strength of the connection. The weight is learned during training.\n",
    "\n",
    "Bias (b): Each neuron has a bias term, which shifts the output of the neuron.\n",
    "\n",
    "Activation Function (f): After calculating the weighted sum of inputs and adding the bias, an activation function (like sigmoid, ReLU, or tanh) is applied to introduce non-linearity. This determines the output of the neuron.\n",
    "\n",
    "Output Layer: After processing through several layers, the final output layer produces the result, which could be a class label (classification) or a continuous value (regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 What is an activation function, and why is it essential in neural\"\n",
    "\n",
    "The mathematical structure of a neural network consists of layers of neurons. Each neuron calculates a weighted sum of its inputs, adds a bias, and then applies an activation function to produce an output. The key components are:\n",
    "\n",
    "Inputs: Data features.\n",
    "Weights (W): Control the strength of connections between neurons.\n",
    "Bias (b): Shifts the output.\n",
    "Activation Function (f): Introduces non-linearity.\n",
    "Output: The final result.\n",
    "\n",
    "Mathematically, the output of a neuron is:\n",
    "\n",
    "y=f(Wâ‹…x+b)\n",
    "\n",
    "Where \n",
    "ğ‘¥\n",
    "x is the input, \n",
    "ğ‘Š\n",
    "W is the weight, \n",
    "ğ‘\n",
    "b is the bias, and \n",
    "ğ‘“\n",
    "f is the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. What is a multilayer neural network!\n",
    "\n",
    "A **multilayer neural network** is a type of neural network that consists of multiple layers of neurons: \n",
    "\n",
    "1. **Input Layer**: Receives the raw data.\n",
    "2. **Hidden Layers**: Intermediate layers between the input and output layers, where the network learns complex patterns. These layers contain multiple neurons and apply activation functions.\n",
    "3. **Output Layer**: Produces the final prediction or classification.\n",
    "\n",
    "The layers are fully connected, meaning each neuron in one layer is connected to every neuron in the next layer. This structure allows the network to learn hierarchical and complex representations of data, making it effective for tasks like image recognition, speech processing, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.What is a loss function, and why is it crucial for neural network training!\n",
    "A loss function is a mathematical function used to measure the difference between the predicted output of a neural network and the actual target values. It quantifies how well or poorly the model is performing.\n",
    "\n",
    " Why it's crucial:\n",
    "- The loss function guides the training process by providing feedback to the model on how to adjust its weights.\n",
    "- During training, the goal is to minimize the loss by optimizing the model's parameters, typically using gradient descent.\n",
    "\n",
    "In short, the loss function acts as a measure of error that helps the neural network learn and improve its predictions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9.How does a neural network learn ?\n",
    "Forward Propagation: Input data is passed through the network, layer by layer, to generate predictions.\n",
    "\n",
    "Loss Calculation: The predicted output is compared to the actual target values using a loss function, which calculates the error.\n",
    "\n",
    "Backpropagation: The error is propagated backward through the network. Gradients (partial derivatives of the loss with respect to the weights) are computed using the chain rule of calculus.\n",
    "\n",
    "Weight Update: Using an optimization algorithm like gradient descent, the weights are adjusted to minimize the loss by moving in the direction of the negative gradient.\n",
    "\n",
    "Iteration: This process repeats for many iterations (epochs) with different batches of data, gradually improving the network's ability to make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.What is an optimizer in neural networks, and why is it necessary?\n",
    "Efficient Learning: Optimizers help find the optimal weights that minimize the loss function, allowing the network to learn effectively.\n",
    "Convergence: They ensure the model converges (reaches a minimum) and doesn't get stuck in suboptimal points.\n",
    "Faster Training: Good optimizers accelerate the training process by choosing the best direction and step size for updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.1 Could you briefly describe some common optimizers?\n",
    "Here are some common optimizers used in neural networks:\n",
    "\n",
    "1. **Stochastic Gradient Descent (SGD)**:\n",
    "   - Updates weights based on the gradient of the loss function with respect to the parameters, using small, random batches of data.\n",
    "   - Simple and widely used but can be slow and prone to getting stuck in local minima.\n",
    "\n",
    "2. **Adam (Adaptive Moment Estimation)**:\n",
    "   - Combines the benefits of **Momentum** and **RMSprop**, adjusting the learning rate for each parameter based on first and second moments (mean and variance) of the gradients.\n",
    "   - Efficient, popular, and works well for most tasks with minimal tuning.\n",
    "\n",
    "3. **RMSprop (Root Mean Square Propagation)**:\n",
    "   - Adapts the learning rate for each parameter by dividing by the moving average of recent gradients.\n",
    "   - Helps deal with noisy gradients and often used in recurrent neural networks (RNNs).\n",
    "\n",
    "4. **Momentum**:\n",
    "   - Adds a fraction of the previous weight update to the current update, helping accelerate learning and reduce oscillations.\n",
    "   - Useful for speeding up training and stabilizing updates.\n",
    "\n",
    "These optimizers help fine-tune the learning process, speeding up convergence and improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12.Can you explain forward and backward propagation in a neural network?\n",
    "### Forward Propagation:\n",
    "1. **Input Data** is passed through the network layer by layer.\n",
    "2. Each neuron processes the input, applies weights, adds biases, and passes the result through an **activation function**.\n",
    "3. This process continues through all layers, generating the **final output** or prediction of the network.\n",
    "\n",
    "### Backward Propagation:\n",
    "1. After forward propagation, the **loss** (error) between the predicted output and the actual target is calculated using a **loss function**.\n",
    "2. The error is then **propagated backward** through the network to calculate the gradients (partial derivatives) of the loss with respect to the weights and biases using the **chain rule**.\n",
    "3. The **weights and biases** are updated using an optimizer (like SGD or Adam) to minimize the loss, and this process is repeated for multiple iterations (epochs) to improve the model.\n",
    "\n",
    "In short, forward propagation makes predictions, and backward propagation adjusts the network's parameters to reduce errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#13. What is weight initialization, and how does it impact training?\n",
    "**Weight initialization** refers to the process of setting the initial values of the weights in a neural network before training begins. Proper initialization helps ensure the network trains effectively and avoids issues like slow convergence or getting stuck in suboptimal solutions.\n",
    "\n",
    "### Impact on Training:\n",
    "1. **Avoiding Vanishing/Exploding Gradients**: Proper initialization helps prevent the gradients from becoming too small (vanishing) or too large (exploding), which can hinder learning.\n",
    "2. **Faster Convergence**: Good initialization can lead to faster and more stable training by starting the optimization process from a reasonable point.\n",
    "3. **Breaking Symmetry**: Randomly initializing weights prevents all neurons from learning the same features, allowing the network to learn diverse representations.\n",
    "\n",
    "Common initialization methods include **Xavier/Glorot** (for sigmoid/tanh) and **He initialization** (for ReLU). These techniques help set the right scale for the weights based on the activation function used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14.What is the vanishing gradient problem in deep learning?\n",
    "\n",
    "The **vanishing gradient problem** occurs during the training of deep neural networks, particularly with activation functions like **sigmoid** or **tanh**. It happens when the gradients (used to update the weights) become very small as they are propagated backward through the network.\n",
    "\n",
    "### Impact:\n",
    "- As the gradients become smaller, the weights in the earlier layers of the network receive very little adjustment, causing the network to learn slowly or fail to learn at all.\n",
    "- This is especially problematic in **deep networks** with many layers, where the gradient can \"vanish\" as it moves backward, making the training process inefficient.\n",
    "\n",
    "### Solution:\n",
    "- Use activation functions like **ReLU** or techniques like **batch normalization** or **better weight initialization** to mitigate the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15.What is the exploding gradient problem?\n",
    "The **exploding gradient problem** occurs when gradients during backpropagation become excessively large, causing the weights to update too drastically. This can result in unstable training, where the model's parameters grow uncontrollably, leading to **overflow** or **NaN (Not a Number)** values.\n",
    "\n",
    "### Impact:\n",
    "- Causes unstable learning, making the model fail to converge or causing the training to diverge.\n",
    "- Often occurs in **deep networks** or with large learning rates.\n",
    "\n",
    "### Solution:\n",
    "- Use techniques like **gradient clipping**, proper **weight initialization**, or lower **learning rates** to prevent gradients from growing too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0] => Prediction: 1\n",
      "Input: [0 1] => Prediction: 1\n",
      "Input: [1 0] => Prediction: 0\n",
      "Input: [1 1] => Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "#1.How do you create a simple perceptron for basic binary classification?\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Initialize weights and bias\n",
    "weights = np.random.rand(2)  # for two input features\n",
    "bias = np.random.rand(1)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Step 2: Activation function (Step function)\n",
    "def activation(z):\n",
    "    return 1 if z >= 0 else 0\n",
    "\n",
    "# Step 3: Training loop\n",
    "def train(X, y, epochs):\n",
    "    global weights, bias\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(X)):\n",
    "            # Forward propagation\n",
    "            z = np.dot(X[i], weights) + bias\n",
    "            y_hat = activation(z)\n",
    "            \n",
    "            # Calculate error and update weights\n",
    "            error = y[i] - y_hat\n",
    "            weights += learning_rate * error * X[i]\n",
    "            bias += learning_rate * error\n",
    "\n",
    "# Example training data (XOR problem for illustration)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features\n",
    "y = np.array([0, 1, 1, 0])  # Target labels\n",
    "\n",
    "# Train the perceptron\n",
    "train(X, y, 100)\n",
    "\n",
    "# Test the perceptron\n",
    "for i in range(len(X)):\n",
    "    z = np.dot(X[i], weights) + bias\n",
    "    print(f\"Input: {X[i]} => Prediction: {activation(z)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ram sharma\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.7231\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.2500 - loss: 0.7215\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.2500 - loss: 0.7199\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.2500 - loss: 0.7184\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.2500 - loss: 0.7169\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2500 - loss: 0.7154\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.2500 - loss: 0.7140\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.2500 - loss: 0.7125\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.2500 - loss: 0.7111\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.2500 - loss: 0.7097\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.2500 - loss: 0.7082\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.2500 - loss: 0.7068\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.2500 - loss: 0.7054\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.2500 - loss: 0.7041\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.2500 - loss: 0.7027\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.2500 - loss: 0.7014\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.2500 - loss: 0.7000\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.2500 - loss: 0.6987\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.2500 - loss: 0.6974\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.2500 - loss: 0.6961\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.2500 - loss: 0.6948\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.2500 - loss: 0.6936\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.2500 - loss: 0.6923\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.2500 - loss: 0.6911\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.2500 - loss: 0.6899\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.2500 - loss: 0.6887\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.2500 - loss: 0.6875\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2500 - loss: 0.6863\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.2500 - loss: 0.6851\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.2500 - loss: 0.6840\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.2500 - loss: 0.6828\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2500 - loss: 0.6817\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.2500 - loss: 0.6806\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.2500 - loss: 0.6795\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.2500 - loss: 0.6784\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.2500 - loss: 0.6773\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.2500 - loss: 0.6763\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.2500 - loss: 0.6752\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.2500 - loss: 0.6742\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.2500 - loss: 0.6731\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2500 - loss: 0.6721\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2500 - loss: 0.6711\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.2500 - loss: 0.6701\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.2500 - loss: 0.6692\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2500 - loss: 0.6682\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.2500 - loss: 0.6672\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2500 - loss: 0.6663\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.2500 - loss: 0.6654\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.2500 - loss: 0.6644\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.2500 - loss: 0.6635\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2500 - loss: 0.6626\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.2500 - loss: 0.6617\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2500 - loss: 0.6608\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.2500 - loss: 0.6600\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.2500 - loss: 0.6591\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.2500 - loss: 0.6583\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.2500 - loss: 0.6574\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.2500 - loss: 0.6566\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.2500 - loss: 0.6558\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.2500 - loss: 0.6550\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.5000 - loss: 0.6541\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.5000 - loss: 0.6534\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - accuracy: 0.5000 - loss: 0.6526\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5000 - loss: 0.6518\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5000 - loss: 0.6510\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5000 - loss: 0.6503\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5000 - loss: 0.6495\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.5000 - loss: 0.6487\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5000 - loss: 0.6480\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.5000 - loss: 0.6473\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.5000 - loss: 0.6465\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5000 - loss: 0.6458\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.5000 - loss: 0.6451\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.5000 - loss: 0.6444\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.5000 - loss: 0.6437\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5000 - loss: 0.6430\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.5000 - loss: 0.6423\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.5000 - loss: 0.6417\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.5000 - loss: 0.6410\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.5000 - loss: 0.6403\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.5000 - loss: 0.6397\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5000 - loss: 0.6390\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5000 - loss: 0.6384\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.5000 - loss: 0.6377\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.5000 - loss: 0.6371\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.5000 - loss: 0.6365\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.5000 - loss: 0.6359\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.5000 - loss: 0.6352\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.5000 - loss: 0.6346\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.5000 - loss: 0.6340\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5000 - loss: 0.6334\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.5000 - loss: 0.6328\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.5000 - loss: 0.6322\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5000 - loss: 0.6316\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.5000 - loss: 0.6311\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.5000 - loss: 0.6305\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.5000 - loss: 0.6299\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5000 - loss: 0.6293\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.5000 - loss: 0.6287\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5000 - loss: 0.6282\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.5000 - loss: 0.6276\n",
      "Loss: 0.6276358962059021, Accuracy: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Predictions: [[0.5184871 ]\n",
      " [0.4786902 ]\n",
      " [0.56281966]\n",
      " [0.37388667]]\n"
     ]
    }
   ],
   "source": [
    "#2 How can you build a neural network with one hidden layer using Keras?\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features\n",
    "y = np.array([0, 1, 1, 0])  # Target labels\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(8, input_dim=2, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=4)\n",
    "\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "predictions = model.predict(X)\n",
    "print(f\"Predictions: {predictions}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ram sharma\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 978ms/step - accuracy: 0.5000 - loss: 0.7192\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2500 - loss: 0.7181\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2500 - loss: 0.7169\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.2500 - loss: 0.7157\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2500 - loss: 0.7146\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.2500 - loss: 0.7135\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.2500 - loss: 0.7124\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2500 - loss: 0.7113\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2500 - loss: 0.7102\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.2500 - loss: 0.7091\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2500 - loss: 0.7080\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.2500 - loss: 0.7069\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2500 - loss: 0.7059\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.2500 - loss: 0.7049\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.2500 - loss: 0.7038\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.2500 - loss: 0.7028\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.2500 - loss: 0.7018\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.2500 - loss: 0.7008\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.2500 - loss: 0.6999\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.2500 - loss: 0.6989\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.2500 - loss: 0.6980\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2500 - loss: 0.6971\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.2500 - loss: 0.6961\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.2500 - loss: 0.6953\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.2500 - loss: 0.6947\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.2500 - loss: 0.6940\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.2500 - loss: 0.6933\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.2500 - loss: 0.6927\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.2500 - loss: 0.6920\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.2500 - loss: 0.6914\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.2500 - loss: 0.6908\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.2500 - loss: 0.6901\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2500 - loss: 0.6895\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.2500 - loss: 0.6889\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.2500 - loss: 0.6884\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.2500 - loss: 0.6878\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2500 - loss: 0.6872\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.2500 - loss: 0.6866\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.2500 - loss: 0.6861\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.2500 - loss: 0.6855\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.2500 - loss: 0.6850\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.2500 - loss: 0.6845\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.2500 - loss: 0.6839\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.2500 - loss: 0.6834\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.2500 - loss: 0.6829\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.2500 - loss: 0.6824\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2500 - loss: 0.6819\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.2500 - loss: 0.6814\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2500 - loss: 0.6810\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5000 - loss: 0.6805\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.5000 - loss: 0.6800\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5000 - loss: 0.6796\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5000 - loss: 0.6791\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5000 - loss: 0.6787\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.5000 - loss: 0.6783\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5000 - loss: 0.6778\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.5000 - loss: 0.6774\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.5000 - loss: 0.6770\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5000 - loss: 0.6766\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5000 - loss: 0.6762\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5000 - loss: 0.6758\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5000 - loss: 0.6754\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5000 - loss: 0.6750\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.5000 - loss: 0.6746\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.5000 - loss: 0.6742\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.5000 - loss: 0.6739\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.5000 - loss: 0.6735\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5000 - loss: 0.6731\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.5000 - loss: 0.6728\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.5000 - loss: 0.6724\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.5000 - loss: 0.6721\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.5000 - loss: 0.6717\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5000 - loss: 0.6714\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5000 - loss: 0.6711\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5000 - loss: 0.6707\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5000 - loss: 0.6704\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5000 - loss: 0.6701\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.5000 - loss: 0.6698\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - accuracy: 0.5000 - loss: 0.6695\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.5000 - loss: 0.6691\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.5000 - loss: 0.6688\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5000 - loss: 0.6685\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.5000 - loss: 0.6682\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.5000 - loss: 0.6679\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.5000 - loss: 0.6676\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.5000 - loss: 0.6673\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.5000 - loss: 0.6670\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5000 - loss: 0.6667\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.5000 - loss: 0.6664\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5000 - loss: 0.6662\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.5000 - loss: 0.6659\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.5000 - loss: 0.6656\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.5000 - loss: 0.6653\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.5000 - loss: 0.6650\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.5000 - loss: 0.6648\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.5000 - loss: 0.6645\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.5000 - loss: 0.6642\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.5000 - loss: 0.6639\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.5000 - loss: 0.6637\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.5000 - loss: 0.6634\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.5000 - loss: 0.6631\n",
      "Loss: 0.6631279587745667, Accuracy: 0.5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "Predictions: [[0.5195884 ]\n",
      " [0.47942775]\n",
      " [0.5408535 ]\n",
      " [0.43426573]]\n"
     ]
    }
   ],
   "source": [
    "#3.How do you initialize weights using the Xavier (Glorot) initialization method in Keras?\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import GlorotUniform  # Xavier initialization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features\n",
    "y = np.array([0, 1, 1, 0])  # Target labels\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(8, input_dim=2, activation='relu', kernel_initializer=GlorotUniform()))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer=GlorotUniform()))\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=4)\n",
    "\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "predictions = model.predict(X)\n",
    "print(f\"Predictions: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ram sharma\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.6258\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 1.0000 - loss: 0.6249\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 0.6239\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.6229\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.6219\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.6209\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.6199\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 1.0000 - loss: 0.6189\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.6179\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 1.0000 - loss: 0.6169\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 0.6159\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 0.6149\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 0.6139\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.6129\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.6118\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.6108\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.6098\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.6087\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.6077\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.6066\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 1.0000 - loss: 0.6056\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.6045\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.6035\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 1.0000 - loss: 0.6024\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.6013\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 0.6003\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.5992\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.5981\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.5970\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 0.5959\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 1.0000 - loss: 0.5947\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.5936\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.5925\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 0.5914\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.5902\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 1.0000 - loss: 0.5891\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.5879\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.5867\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.5856\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 1.0000 - loss: 0.5844\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.5832\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.5820\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.5808\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.5796\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 1.0000 - loss: 0.5783\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 1.0000 - loss: 0.5771\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.5759\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 0.5746\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.5734\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 1.0000 - loss: 0.5721\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.5708\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step - accuracy: 1.0000 - loss: 0.5695\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 0.5683\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 1.0000 - loss: 0.5669\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 0.5656\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 0.5643\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.5630\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 1.0000 - loss: 0.5616\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.5603\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.5589\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.5576\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.5562\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.5548\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.5534\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.5520\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.5506\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.5492\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.5477\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 1.0000 - loss: 0.5463\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.5448\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 1.0000 - loss: 0.5434\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.5419\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 1.0000 - loss: 0.5404\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.5390\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.5375\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.5359\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.5344\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.5329\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.5313\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 0.5298\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.5282\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 0.5267\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.5251\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.5235\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 1.0000 - loss: 0.5219\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.5203\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.5187\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.5170\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.5154\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.5138\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.5121\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.5105\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 0.5088\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.5071\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.5054\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 1.0000 - loss: 0.5037\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 0.5020\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.5003\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.4985\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.4968\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - accuracy: 1.0000 - loss: 0.4950\n",
      "Loss: 0.49504518508911133, Accuracy: 1.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step\n",
      "Predictions: [[0.41109246]\n",
      " [0.6179893 ]\n",
      " [0.6082705 ]\n",
      " [0.376418  ]]\n"
     ]
    }
   ],
   "source": [
    "#4.How can you apply different activation functions in a neural network in Keras?\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features\n",
    "y = np.array([0, 1, 1, 0])  # Target labels\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(8, input_dim=2, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(8, activation='tanh'))\n",
    "\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=4)\n",
    "\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "predictions = model.predict(X)\n",
    "print(f\"Predictions: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.7500 - loss: 0.7310\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step - accuracy: 0.2500 - loss: 0.7177\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.5989\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.2500 - loss: 1.0381\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.5000 - loss: 0.8931\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7500 - loss: 0.6429\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5000 - loss: 0.7771\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5000 - loss: 0.7368\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.5000 - loss: 0.6381\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7500 - loss: 0.7210\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7500 - loss: 0.7252\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7500 - loss: 0.5113\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5000 - loss: 0.8169\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.6057\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.5000 - loss: 0.7460\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.7500 - loss: 0.5694\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.5000 - loss: 0.7279\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7500 - loss: 0.6405\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.5000 - loss: 0.8159\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7500 - loss: 0.6713\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5000 - loss: 0.8902\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.6170\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7500 - loss: 0.5904\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.2500 - loss: 0.7687\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7500 - loss: 0.5116\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5000 - loss: 0.6405\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.7500 - loss: 0.7339\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.7500 - loss: 0.6612\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7500 - loss: 0.5301\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.5007\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7500 - loss: 0.6862\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5000 - loss: 0.6853\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7500 - loss: 0.5575\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5000 - loss: 0.6402\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7500 - loss: 0.7326\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 0.5934\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.5000 - loss: 0.6676\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.5000 - loss: 0.7229\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.7500 - loss: 0.6747\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.2500 - loss: 0.8312\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.5000 - loss: 0.7550\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5000 - loss: 0.7170\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7500 - loss: 0.5851\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7500 - loss: 0.5968\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5000 - loss: 0.6964\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7500 - loss: 0.7659\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.5000 - loss: 0.7186\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.2500 - loss: 0.7393\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5000 - loss: 0.6070\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.2500 - loss: 0.8257\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7500 - loss: 0.5709\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7500 - loss: 0.8550\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5000 - loss: 0.7150\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 0.4735\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2500 - loss: 0.7102\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7500 - loss: 0.5969\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6497\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5000 - loss: 0.7572\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7500 - loss: 0.7020\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.6722\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.5715\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7500 - loss: 0.5775\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7500 - loss: 0.5507\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.7500 - loss: 0.5898\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5000 - loss: 0.7521\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.7500 - loss: 0.6140\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.5000 - loss: 0.7482\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.7500 - loss: 0.5691\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.7500 - loss: 0.6347\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.5000 - loss: 0.9190\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7500 - loss: 0.6496\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.4568\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7500 - loss: 0.8138\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.5816\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5000 - loss: 1.0072\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5000 - loss: 0.7348\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.7500 - loss: 0.8067\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.5000 - loss: 0.8167\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.2500 - loss: 0.8659\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.5000 - loss: 0.6464\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.7500 - loss: 0.6015\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.7500 - loss: 0.7122\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.5000 - loss: 0.9618\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.2500 - loss: 0.8493\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.5000 - loss: 0.7564\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.2500 - loss: 0.7621\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7500 - loss: 0.5826\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2500 - loss: 0.8832\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.2500 - loss: 0.7743\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.5881\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.5000 - loss: 0.7536\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.5000 - loss: 0.6952\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.5000 - loss: 0.7043\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.5000 - loss: 0.6717\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.2500 - loss: 0.9404\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7500 - loss: 0.5425\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7500 - loss: 0.6555\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.5221\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5000 - loss: 0.6124\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5000 - loss: 0.8030\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - accuracy: 1.0000 - loss: 0.6542\n",
      "Loss: 0.6542092561721802, Accuracy: 1.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "Predictions: [[0.4890244 ]\n",
      " [0.554782  ]\n",
      " [0.5061997 ]\n",
      " [0.49104694]]\n"
     ]
    }
   ],
   "source": [
    "#5.How do you add dropout to a neural network model to prevent overfitting?\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features\n",
    "y = np.array([0, 1, 1, 0])  # Target labels\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(8, input_dim=2, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(8, activation='tanh'))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=4)\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "predictions = model.predict(X)\n",
    "print(f\"Predictions: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the neural network (after forward propagation):\n",
      "[[0.81034425]\n",
      " [0.90986012]\n",
      " [0.88720391]\n",
      " [0.94893169]]\n"
     ]
    }
   ],
   "source": [
    "#6.How do you manually implement forward propagation in a simple neural network?\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the input data and initialize weights and biases\n",
    "\n",
    "# Example data (XOR problem)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features (4 examples, 2 features)\n",
    "y = np.array([0, 1, 1, 0])  # Target labels (XOR output)\n",
    "\n",
    "# Initialize weights and biases for the input to hidden layer (2 input features, 2 neurons in hidden layer)\n",
    "W1 = np.random.rand(2, 2)  # Weights for input layer to hidden layer\n",
    "b1 = np.random.rand(1, 2)  # Biases for hidden layer\n",
    "\n",
    "# Initialize weights and biases for the hidden to output layer (2 neurons in hidden layer, 1 output neuron)\n",
    "W2 = np.random.rand(2, 1)  # Weights for hidden layer to output layer\n",
    "b2 = np.random.rand(1, 1)  # Bias for output layer\n",
    "\n",
    "# Step 2: Define the activation function (Sigmoid for output, ReLU for hidden)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "\n",
    "Z1 = np.dot(X, W1) + b1  \n",
    "A1 = relu(Z1)  \n",
    "\n",
    "Z2 = np.dot(A1, W2) + b2  \n",
    "A2 = sigmoid(Z2)  \n",
    "\n",
    "\n",
    "print(\"Output of the neural network (after forward propagation):\")\n",
    "print(A2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ram sharma\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.7500 - loss: 0.5811\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.7500 - loss: 0.5466\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.7500 - loss: 0.5139\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7500 - loss: 0.4829\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7500 - loss: 0.4541\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.7500 - loss: 0.4277\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.7500 - loss: 0.4038\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7500 - loss: 0.3823\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.7500 - loss: 0.3629\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.3455\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 1.0000 - loss: 0.3299\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step - accuracy: 1.0000 - loss: 0.3157\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.3054\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 0.2988\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.2923\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.2861\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 0.2800\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.2740\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.2681\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.2623\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.2567\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 0.2512\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.2459\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.2406\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.2355\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.2306\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.2257\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.2209\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.2163\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.2118\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.2074\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 0.2031\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.1989\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.1948\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 0.1909\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 1.0000 - loss: 0.1870\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 1.0000 - loss: 0.1833\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.1797\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.1761\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.1727\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.1694\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.1662\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.1630\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.1599\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 1.0000 - loss: 0.1566\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.1533\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.1499\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 0.1466\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.1433\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.1400\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.1367\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.1334\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.1301\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.1269\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.1237\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 1.0000 - loss: 0.1206\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.1174\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - accuracy: 1.0000 - loss: 0.1144\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.1113\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.1084\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.1055\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.1027\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 0.1000\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 1.0000 - loss: 0.0974\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.0949\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 1.0000 - loss: 0.0926\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0904\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0884\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0865\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 1.0000 - loss: 0.0847\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0830\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 0.0815\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 0.0800\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 0.0787\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0774\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step - accuracy: 1.0000 - loss: 0.0763\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.0752\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 1.0000 - loss: 0.0743\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.0733\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.0724\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.0716\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0708\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.0700\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.0693\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0686\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0679\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.0673\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0667\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.0661\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.0655\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.0650\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 0.0645\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0640\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0635\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0630\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.0626\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0621\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0617\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0612\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 0.0608\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.5000 - loss: 0.5807\n",
      "Loss: 0.5807257890701294, Accuracy: 0.5\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000017CDD41C4A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "Predictions: [[0.37602985]\n",
      " [0.49755314]\n",
      " [0.6709665 ]\n",
      " [0.5295951 ]]\n"
     ]
    }
   ],
   "source": [
    "#7.How do you add batch normalization to a neural network model in Keras?\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, input_dim=2, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=4)\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "predictions = model.predict(X)\n",
    "print(f\"Predictions: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ram sharma\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#8.How can you visualize the training process with accuracy and loss curves?\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Example data (XOR problem)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features\n",
    "y = np.array([0, 1, 1, 0])  # Target labels\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add hidden layers and output layer\n",
    "model.add(Dense(8, input_dim=2, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and save the training history\n",
    "history = model.fit(X, y, epochs=100, batch_size=4, verbose=0)\n",
    "\n",
    "# Plot training loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
